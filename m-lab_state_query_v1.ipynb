{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate SQL query string, then read Google BigQuery to access M-Lab data and get ISP name using mlabnetdb\n",
    "\n",
    "Created by John Burt, for allTBD group.\n",
    "\n",
    "This notebook is a demo showing how to generate a query based on Kinga's R based query script to access M-Lab data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import calendar\n",
    "\n",
    "# This function takes as input mlab_location, AS number, \n",
    "# start_time, end_time and the optional country (the default \n",
    "# country is set to US)\n",
    "# The choices for the metric are: \"dtp\", \"rtt\", and \"prt\" for download \n",
    "# throughput, round trip time and packet retransmission respectively\n",
    "# The start_time, end_time info should be entered in the 'yyyy-mm-dd' format\n",
    "# The output of the function, when successful, is a string\n",
    "\n",
    "# input parameters:\n",
    "#   area_codes = list of 3 digit client area codes to search\n",
    "#   metrics = list of metric types to collect (\"dtp\", \"rtt\", \"prt\"). Each record\n",
    "#             must contain a valid value for the metrics passed, so entering multiple\n",
    "#             metrics may reduce the number of records acquired.\n",
    "#   start_time = earliest date for acquired data, format 'yyyy-mm-dd' \n",
    "#   end_time = latest date for acquired data, format 'yyyy-mm-dd' \n",
    "#   country = country to search (only tested with 'US')\n",
    "\n",
    "# output:\n",
    "#   query string to pass to google big query\n",
    "        \n",
    "\n",
    "def query_writer_by_area_code(area_codes, metrics, start_time, end_time, country = 'US' ): \n",
    "      \n",
    "    #DEFINING THE BASIC QUERIES FOR EACH METRIC\n",
    "\n",
    "    #The basic query for download throughput\n",
    "    basic = (\"#standardSQL\"\n",
    "        \"\\nSELECT \"\n",
    "        \"\\n  web100_log_entry.log_time AS log_time, \"\n",
    "        \"\\n  connection_spec.client_geolocation.city  AS client_city,  \"\n",
    "        \"\\n  connection_spec.client_geolocation.area_code AS client_area_code,  \"\n",
    "        \"\\n  web100_log_entry.connection_spec.remote_ip AS client_ip, \"\n",
    "        \"\\n  web100_log_entry.connection_spec.local_ip AS MLab_ip, \"\n",
    "        \"\\n  connection_spec.client_geolocation.latitude AS client_latitude, \"\n",
    "        \"\\n  connection_spec.client_geolocation.longitude AS client_longitude, \"\n",
    "        \"\\n  connection_spec.client_geolocation.postal_code AS client_postal_code, \"\n",
    "        \"\\n  connection_spec.server_geolocation.latitude AS MLab_latitude, \"\n",
    "        \"\\n  connection_spec.server_geolocation.longitude AS MLab_longitude, \"\n",
    "        \"\\n  connection_spec.server_geolocation.postal_code AS MLab_postal_code, \"\n",
    "              )\n",
    "    \n",
    "    metrics_str1 = []\n",
    "    metrics_str2 = []\n",
    "    \n",
    "    if type(metrics) == str:\n",
    "        metrics = [metrics]\n",
    "        \n",
    "    for metric in metrics:\n",
    "        if \"dtp\" in metric:\n",
    "            metrics_str1.append(\"\\n  8 * (web100_log_entry.snap.HCThruOctetsAcked / \"\n",
    "                \"\\n      (web100_log_entry.snap.SndLimTimeRwin + \"\n",
    "                \"\\n       web100_log_entry.snap.SndLimTimeCwnd + \"\n",
    "                \"\\n       web100_log_entry.snap.SndLimTimeSnd)) AS download_Mbps\")\n",
    "            metrics_str2.append(\"\\n  AND web100_log_entry.snap.CongSignals > 0\")\n",
    "            \n",
    "        if \"rtt\" in metric:\n",
    "            metrics_str1.append(\"\\n  web100_log_entry.snap.MinRTT AS min_rtt\" )\n",
    "            metrics_str2.append(\"\\n  AND web100_log_entry.snap.CountRTT > 10\")\n",
    "            \n",
    "        if \"prt\" in metric:\n",
    "            metrics_str1.append(\"\\n  (web100_log_entry.snap.SegsRetrans / web100_log_entry.snap.DataSegsOut) AS packet_retransmission_rate\")\n",
    "            metrics_str2.append(\"\\n  AND web100_log_entry.snap.DataSegsOut > 0\")\n",
    "    \n",
    "    for s in metrics_str1[:-1]:\n",
    "        basic += s + \",\"\n",
    "    basic += metrics_str1[-1]\n",
    "    \n",
    "    basic += (\"\\nFROM \"\n",
    "        \"\\n  `measurement-lab.release.ndt_all` \"\n",
    "        \"\\nWHERE \"\n",
    "        \"\\n  connection_spec.data_direction = 1 \")    \n",
    "\n",
    "    for s in metrics_str2:\n",
    "        basic += s\n",
    "    \n",
    "    # area code condition area_codes\n",
    "    ac_var = \"connection_spec.client_geolocation.area_code\"\n",
    "    ac_cond = \"\\n  AND (\"\n",
    "    for ac in area_codes[:-1]:\n",
    "        ac_cond += ac_var + \"=\" + ac + \" OR \"\n",
    "    ac_cond += ac_var + \"=\" + str(area_codes[-1:][0]) + \")\"\n",
    "    #print(ac_cond)\n",
    "    \n",
    "    #WRITING THE TIME CONDITION\n",
    "    tstamp_var = \"partition_date \"\n",
    "    tframe_cond = ('\\n  AND ' + tstamp_var + '> \"' + start_time +\n",
    "        '\"\\n  AND ' + tstamp_var + '< \"' + end_time + '\"')\n",
    "    #print(tframe_cond)\n",
    "\n",
    "    #WRITING THE COUNTRY CONDITION\n",
    "    country_string = \"'\" + country + \"'\" \n",
    "    country_var = \"connection_spec.client_geolocation.country_code\"\n",
    "    country_cond = \"\\n  AND \" + country_var + \"=\" + country_string\n",
    "    #print(country_cond)\n",
    "\n",
    "    #WRITING THE QUERY\n",
    "    the_query = basic + ac_cond + tframe_cond\n",
    "    #with open(\"querypy.txt\", \"w\") as text_file:\n",
    "    #    text_file.write(the_query)\n",
    "\n",
    "    return the_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test the query_writer output:\n",
    "# acs = ['201', '202', '203', '204', '205', '206']\n",
    "# metrics = [\"dtp\", \"rtt\", \"prt\"]\n",
    "# the_query = query_writer_by_area_code(acs, metrics, \"2010-01-01\",\"2019-01-01\")\n",
    "# print(the_query)\n",
    "\n",
    "# metrics = [\"rtt\", \"prt\"]\n",
    "# the_query = query_writer_by_area_code(acs, metrics, \"2010-01-01\",\"2019-01-01\")\n",
    "# print(the_query)\n",
    "\n",
    "# metrics = [\"prt\"]\n",
    "# the_query = query_writer_by_area_code(acs, metrics, \"2010-01-01\",\"2019-01-01\")\n",
    "# print(the_query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to acquire m-lab data into a pandas dataframe,   \n",
    "#  then acquire and add ISP name and ASN as df columns\n",
    "\n",
    "from pandas.io import gbq\n",
    "import importlib\n",
    "# add alltbd path so we can import mlabnetdb from there\n",
    "import os\n",
    "import sys\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "\n",
    "import mlabnetdb\n",
    "#importlib.reload(mlabnetdb) \n",
    "from mlabnetdb import *\n",
    "\n",
    "def acquire_mlab_data_by_area_code(project_id, areacodes, metrics, start_time, end_time, country = 'US'):\n",
    "\n",
    "    df = []\n",
    "    \n",
    "    # generate the query\n",
    "    print(\"  getting mlab data from GBQ.....\")\n",
    "    querystring = query_writer_by_area_code(areacodes, metrics, start_time, end_time, country)\n",
    "#     print(querystring)\n",
    "    \n",
    "    # read the query output into a pandas dataframe\n",
    "    #   NOTE: the first time this runs, you will be prompted for an authorization key. \n",
    "    #    Click on the link provided, get the key string, paste it in, and go.\n",
    "    df = gbq.read_gbq(querystring, project_id=project_id, verbose=True, dialect='standard')\n",
    "\n",
    "    # use mlabnetdb to get ISP names\n",
    "    print(\"  getting ISP names.....\")\n",
    "    access_owner = []\n",
    "    access_ispname = []\n",
    "    access_asn = []\n",
    "    access_iperrors = []\n",
    "    transit_owner = []\n",
    "    transit_ispname = []\n",
    "    transit_asn = []\n",
    "    transit_iperrors = []\n",
    "    for access_ip, transit_ip in zip(df.client_ip, df.MLab_ip):\n",
    "        try:\n",
    "            access_ipinfo = lookup(access_ip, date=None)\n",
    "            transit_ipinfo = lookup(transit_ip, date=None)\n",
    "        except:\n",
    "            print(\"    error: for ip %s, lookup error\"%(access_ip,transit_ip))\n",
    "            access_ipinfo = []\n",
    "            transit_ipinfo = []\n",
    "            \n",
    "        if access_ipinfo:\n",
    "            access_owner.append(access_ipinfo['autonomous_system_organization'])\n",
    "            access_asn.append(access_ipinfo['autonomous_system_number'])\n",
    "            access_ispname.append(access_ipinfo['isp'])\n",
    "\n",
    "            transit_owner.append(transit_ipinfo['autonomous_system_organization'])\n",
    "            transit_asn.append(transit_ipinfo['autonomous_system_number'])\n",
    "            transit_ispname.append(transit_ipinfo['isp'])\n",
    "        else:\n",
    "#             print(\"    error: for ip %s, ipinfo==None\"%(ip))\n",
    "            access_owner.append('')\n",
    "            access_asn.append(0)\n",
    "            access_ispname.append('')\n",
    "            access_iperrors.append(access_ip)\n",
    "            \n",
    "            transit_owner.append('')\n",
    "            transit_asn.append(0)\n",
    "            transit_ispname.append('')\n",
    "            transit_iperrors.append(transit_ip)\n",
    "            \n",
    "    print(\"    DONE getting ISP names\")\n",
    "    \n",
    "    if len(access_iperrors) > 0:\n",
    "        print('      Could not find ISP info for ',str(len(access_iperrors)),\n",
    "              ' IPs:', ', '.join(str(x) for x in np.unique(access_iperrors)))\n",
    "\n",
    "    # add mlab search info to dataframe\n",
    "    df[\"area_codes\"] = [','.join(areacodes)] * df.shape[0]\n",
    "\n",
    "    # add IP_owner and IP_ASN columns to the dataframe\n",
    "    df[\"access_IP_owner\"] = access_owner\n",
    "    df[\"access_IP_ASN\"] = access_asn\n",
    "    # get company name from owner string\n",
    "    df[\"access_ISP_name\"] = access_ispname\n",
    "    \n",
    "    df[\"transit_IP_owner\"] = transit_owner\n",
    "    df[\"transit_IP_ASN\"] = transit_asn\n",
    "    # get company name from owner string\n",
    "    df[\"transit_ISP_name\"] = transit_ispname\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to pickle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def pickle_data(data, outputfilename):\n",
    "    # save data\n",
    "    with open(outputfilename, 'wb') as handle:\n",
    "        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(\"m-lab data saved to \",outputfilename)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read and fix up area code data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of area codes for a given city and state\n",
    "areacodelocs = pd.read_csv('areacode_latitude_longitude.csv', index_col=None, encoding='latin_1')\n",
    "\n",
    "# truncate area code pairs to just first 3 digits\n",
    "areacodelocs.areacode = list([s[:3] for s in areacodelocs.areacode])\n",
    "\n",
    "# drop rows with NaNs in important fields\n",
    "areacodelocs = areacodelocs.drop(areacodelocs[areacodelocs.city.isnull()].index)\n",
    "areacodelocs = areacodelocs.drop(areacodelocs[areacodelocs.region.isnull()].index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entry point:\n",
    "\n",
    "acquire mlab data or a list of cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  getting mlab data from GBQ.....\n",
      "Requesting query... ok.\n",
      "Job ID: b8f0f740-e1cc-4e8c-8a9a-021d89846593\n",
      "Query running...\n",
      "  Elapsed 7.78 s. Waiting...\n",
      "  Elapsed 9.01 s. Waiting...\n",
      "  Elapsed 10.24 s. Waiting...\n",
      "  Elapsed 11.47 s. Waiting...\n",
      "  Elapsed 12.7 s. Waiting...\n",
      "  Elapsed 13.92 s. Waiting...\n",
      "  Elapsed 15.15 s. Waiting...\n",
      "  Elapsed 16.38 s. Waiting...\n",
      "  Elapsed 17.61 s. Waiting...\n",
      "  Elapsed 18.84 s. Waiting...\n",
      "  Elapsed 20.07 s. Waiting...\n",
      "  Elapsed 21.29 s. Waiting...\n",
      "  Elapsed 22.52 s. Waiting...\n",
      "  Elapsed 23.75 s. Waiting...\n",
      "  Elapsed 24.98 s. Waiting...\n",
      "  Elapsed 26.21 s. Waiting...\n",
      "  Elapsed 27.44 s. Waiting...\n",
      "  Elapsed 28.67 s. Waiting...\n",
      "  Elapsed 29.9 s. Waiting...\n",
      "  Elapsed 31.23 s. Waiting...\n",
      "  Elapsed 32.66 s. Waiting...\n",
      "  Elapsed 33.89 s. Waiting...\n",
      "  Elapsed 35.12 s. Waiting...\n",
      "  Elapsed 36.35 s. Waiting...\n",
      "  Elapsed 37.58 s. Waiting...\n",
      "Query done.\n",
      "Processed: 14.6 GB Billed: 14.6 GB\n",
      "Standard price: $0.07 USD\n",
      "\n",
      "Retrieving results...\n",
      "Got 2324615 rows.\n",
      "\n",
      "Total time taken 560.06 s.\n",
      "Finished at 2018-05-23 16:48:36.\n",
      "  getting ISP names.....\n",
      "    DONE getting ISP names\n",
      "      Could not find ISP info for  603  IPs: 156.39.96.253, 162.221.179.45, 162.221.180.68, 173.45.193.41, 173.45.199.229, 173.45.202.93, 173.45.204.230, 173.45.209.6, 173.45.212.140, 173.45.214.100, 173.45.214.122, 173.45.215.75, 173.45.218.138, 173.45.220.173, 173.45.222.44, 173.45.223.109, 18.224.1.203, 18.224.1.206, 199.27.191.251, 206.125.157.73, 206.125.159.160, 206.125.159.231, 208.52.169.58, 208.83.37.2, 208.84.125.122, 208.84.127.124, 208.84.127.148, 208.84.127.183, 208.84.127.69, 208.91.33.145, 208.91.33.194, 209.127.128.56, 209.127.129.21, 209.127.132.20, 209.127.132.50, 209.127.133.16, 209.127.135.25, 209.127.16.14, 209.127.197.131, 209.127.197.59, 209.127.21.30, 209.127.210.107, 209.127.215.69, 209.127.218.86, 209.127.222.7, 209.127.23.15, 209.127.29.22, 209.127.31.39, 209.127.43.24, 209.127.43.32, 209.127.46.19, 209.127.50.27, 209.127.50.6, 209.127.61.112, 209.127.61.47, 209.127.61.52, 209.127.71.13, 209.127.72.131, 209.127.73.28, 209.127.74.50, 209.127.88.18, 209.127.88.33, 209.127.88.44, 209.127.97.22, 209.151.103.21, 209.151.103.38, 209.151.123.139, 209.151.123.43, 216.126.16.2, 216.126.17.2, 216.99.199.19, 216.99.213.102, 216.99.215.180, 24.224.55.136, 24.224.58.82, 24.224.59.67, 24.245.72.187, 24.245.72.235, 24.54.150.8, 24.54.150.86, 24.54.151.90, 24.72.186.66, 24.75.124.123, 24.75.124.244, 64.113.24.178, 64.113.24.18, 64.113.24.47, 64.113.24.80, 64.113.25.172, 64.113.26.58, 64.20.208.14, 66.97.113.119, 66.97.117.150, 66.97.120.135, 66.97.120.4, 66.97.121.172, 66.97.122.203, 66.97.123.228, 66.97.123.66, 66.97.125.75, 69.5.54.142, 69.5.54.16, 69.5.54.89, 69.5.55.111, 69.5.60.101, 69.5.60.103, 69.5.60.167, 69.5.60.24\n",
      "  got 2324615 rows\n",
      "saving data for    df.shape= (2324615, 21)\n",
      "m-lab data saved to  mlab_data_mlabnetdb_2014-2015_all_states.pkl\n",
      "Wall time: 19min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "\n",
    "# select a state to collect mlab data for, or all states\n",
    "state = '' # all states\n",
    "# state = 'NY'\n",
    "\n",
    "# set date range for search filter\n",
    "daterange = [\"2014-01-01\",\"2015-01-01\"]\n",
    "\n",
    "# select metrics to collect\n",
    "metrics = [\"dtp\", \"rtt\", \"prt\"]\n",
    "\n",
    "outputfilename = \"mlab_data_mlabnetdb_2014-2015_all_states.pkl\"\n",
    "\n",
    "# this is my project ID, you will probably use a different one\n",
    "project_id = 'mlab-194421'\n",
    "\n",
    "local_acs = np.unique(areacodelocs.areacode[areacodelocs.region.str.contains(state)])\n",
    "df = []\n",
    "if type(df) != pd.core.frame.DataFrame:\n",
    "    df = acquire_mlab_data_by_area_code(project_id, local_acs, metrics, daterange[0], daterange[1])\n",
    "else:\n",
    "    df = pd.concat([df, acquire_mlab_data_by_area_code(project_id, local_acs, metrics, daterange[0], daterange[1])])\n",
    "print(\"  got %d rows\"%(df.shape[0]))\n",
    "\n",
    "print(\"saving data for \",state,\" df.shape=\",df.shape)\n",
    "pickle_data(df, outputfilename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
